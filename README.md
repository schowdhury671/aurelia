# AURELIA : Testâ€‘time Reasoning Distillation in Audioâ€‘Visual LLMs
[ğŸ“„ Paper](https://arxiv.org/abs/2503.23219) Â· [ğŸŒ Project Page](https://schowdhury671.github.io/aurelia_project/)

### AURELIA : Testâ€‘time Reasoning Distillation in Audioâ€‘Visual LLMs is accepted to ICCV 2025!! ğŸ‰ğŸ‰

<p align="center">
  <img src="diagrams/aurelia_banner.png" alt="AURELIA" width="80%"/>
</p>

#### This is the official code for the paper - AURELIA : Testâ€‘time Reasoning Distillation in Audioâ€‘Visual LLMs

We introduce **AURELIA**, a novel actor-critic based audio-visual (AV) reasoning framework that distills structured, step-by-step reasoning into AVLLMs at test time, improving their ability to process complex multi-modal inputs without additional training or fine-tuning. 

---

## Table of Contents
1. [Quick Start](#quick-start)
2. [Generating Reasoning Data](#generating-reasoning-data)
3. [Evaluation](#evaluation)
4. [Citation](#citation)

---

## QuickÂ Start
Clone the repo & install dependencies
```bash
git clone https://github.com/schowdhury671/aurelia
conda create -n aurelia python=3.10 -y
conda activate aurelia
pip install openai==0.28.0
pip install google-generativeai
pip install -q -U pytube moviepy
apt-get install -y ffmpeg
```

To run the multiâ€‘agent pipeline you must export valid keys for both OpenAI and Google Gemini APIs:
```bash
export OPENAI_API_KEY="..."
export GOOGLE_API_KEY="..."
```

---

## GeneratingÂ ReasoningÂ Data

Run the data generation pipeline
```bash
cd data_generation
python gen_data.py --save_path "reason_data.json" \
                   --video_path "sample.mp4" \
                   --audio_path "sample.mp3" \
                   --query "What is the most popular food of the country where the loudest instrument originates from?" \
                   --max_tries 5
```
`gen_data.py` will iteratively call the chosen LLMs, synthesize the reasoning caption, and stop once the evaluator score â‰¥Â Ï„.

---

## Evaluation
We benchmark various AV-LLMs following the settings mentioned in Section 5Â of the paper.

Given below are the links to the checkpoints of the public AV-LLMs.

| Family | Size | Code & Checkpoint |
|--------|------|------------------|
| Videoâ€‘LLaMA | 7â€¯B | <https://github.com/DAMO-NLP-SG/Video-LLaMA> |
| Videoâ€‘LLaMAâ€¯2 | 7â€¯B | <https://github.com/DAMO-NLP-SG/VideoLLaMA2> |
| Unifiedâ€‘IOâ€‘2 | 1B, 3B | <https://github.com/allenai/unified-io-2> |
| PandaGPT | 13â€¯B | <https://github.com/yxuansu/PandaGPT> |
| Macawâ€‘LLM | 7â€¯B | <https://github.com/lyuchenyang/Macaw-LLM> |
| ImageBindâ€‘LLM | 7â€¯B | <https://github.com/OpenGVLab/LLaMA-Adapter/tree/main/imagebind_LLM> |
| Xâ€‘InstructBLIP | 13â€¯B | <https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip> |
| OneLLM | 7â€¯B | <https://github.com/csuhan/OneLLM> |
| CREMA | 4â€¯B | <https://github.com/Yui010206/CREMA> |
| AnyGPT | 7â€¯B | <https://github.com/OpenMOSS/AnyGPT> |
| NExTâ€‘GPT | 7â€¯B | <https://github.com/NExT-GPT/NExT-GPT> |
| VITA | 7â€¯B | <https://github.com/VITA-MLLM/VITA> |
| Bayâ€‘CAT | 7â€¯B | <https://github.com/rikeilong/Bay-CAT> |
| videoâ€‘SALMONN | 7â€¯B | <https://github.com/bytedance/SALMONN/tree/videosalmonn> |
| **Closedâ€‘source** | â€” | [GeminiÂ 1.5Â Pro](https://ai.google.dev/gemini) Â· [RekaÂ Core](https://reka.ai) |

---

## Citation
If you find **AURELIA** useful in your research, please cite:
```bibtex
@article{chowdhury2025aurelia,
  title={AURELIA: Test-time Reasoning Distillation in Audio-Visual LLMs},
  author={Chowdhury, Sanjoy and Ghani, Hanan and Anand, Nishit and Nag, Sayan and Gao, Ruohan and Elhoseiny, Mohamed and Khan, Salman and Manocha, Dinesh},
  journal={arXiv preprint arXiv:2503.23219},
  year={2025}
}
```

